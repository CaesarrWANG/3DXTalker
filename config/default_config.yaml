# video config
fps: 25

# audio config
sample_rate: 16000

batch_size : 128

num_frames: 250

train_data_root_path: '/path/to/trainset'
test_data_root_path: '/path/to/testset'

mouth_indices_path: './resources/02_flame_mouth_idx.npy'
emoca_model_path: "./assets/EMOCA/models"
emoca_model_name: "EMOCA_v2_lr_mse_20"


log_path: './log'
events_path: './log/events'
# MODEL
audio_encoder_repo: 'microsoft/wavlm-base-plus' # choose from 'microsoft/wavlm-base-plus' and 'wav2vec2-large-xlsr-53'
freeze_wav2vec: True 
interpolate_pos: 1
dit_dm: 1024
flame_dim: 284    
n_heads: 12
dropout: 0.1
n_layers: 6
mlp_ratio: 4
n_diffusion_train_steps: 512   # 512
n_diffusion_inference_steps: 32  # 8

# General training setting
lr: 1e-4
num_epochs: 100
seed: 1234
checkpointing_steps: 100
num_workers: 24
clip_grad_norm: 1.0
eval_every: 1
dtype: "bf16"
mixed_precision: "bf16"  # choices=["no", "fp16", "bf16", "fp8"]
output_dir: "./checkpoints_output"
resume_from_checkpoint:  "None"   #"latest"
load_from_checkpoint: null
debugpy_port: 28002
only_run_log_validation: false
debug: false
cpu: false
optimizer: "adamW"



